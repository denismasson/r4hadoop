{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModOP Création d'un env isolé pour PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans l'environnement virtuel on crée un répertoire, dans ce même répertoire il faut descendre les paquets que l'on souhaite \n",
    "rendre disponible sur le cluster, par exemple pour le paquet numpy:\n",
    "pip download numpy tous les paquets qui serons descendu dans le répertoire serons potentiellement de deux types:\n",
    "- whl (wheel)\n",
    "- tar.gz\n",
    "\n",
    "Dans le cas ou nous avons un *__tar.gz__* il faut générer un *__whl__*, pour cela il faut untar l'archive se placer dans le répertoire et lancé la commande  \n",
    "__python setup.py bdist_wheel__.\n",
    "\n",
    "Une fois tous les fichiers __Wheel__ générés, il faut dézipper les fichiers avec __unzip package.whl__ .\n",
    "\n",
    "__ATTENTION !!!__ : certain package nécessitent des fichiers système inhérent à l'architecture cible qui ne peuvent malheureusement pas être portés par le package, cela nécessite l'intervention d'un admin afin d'installer ces dépendances.\n",
    "\n",
    "Une fois l'environnement crée, mettre ce répertoire sur HDFS par exemple :\n",
    "\n",
    "__hdfs dfs -put /tmp/envs/py27PySpark/venv /applis/hadd/produits/anaconda2/envs/py27PySpark/venv__\n",
    "\n",
    "Penser aux droits sur le répertoire !\n",
    "\n",
    "##### *Remarques*:\n",
    "Dans un contexte ou du PySpark est executer via jupyter, on est dans une configuration yarn client de ce faite le driver est l'edge sur laquelle vous avez lancez jupyter, vous pouvez tout aussi bien vous passer la partie mise sur HDFS et pointer directement sur le répertoire __/applis/hadd/produits/anaconda2/envs/py27PySpark/venv__ (par exemple).\n",
    "\n",
    "Par contre si vous lancez votre job via __*spark-submit*__ en yarn cluster le driver n'étant pas l'edge d'exécution, le passage par HDFS est indispensable pour l'instant. Effectivement à terme nous souhaitons faire passer l'environnement dans une archive que nous soumettrons via une option de configuration Spark *--archive*, mais cette méthode n'a pas encore été éprouvée.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENV_DIR=\"/tmp/envs/py27PySpark/venv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_path_names(path):\n",
    "    from pyspark.context import SparkContext\n",
    "    \"\"\"List files and directories in an HDFS path\n",
    "\n",
    "    Args:\n",
    "        path (str): HDFS path to directory\n",
    "\n",
    "    Returns:\n",
    "        [str]: list of file/directory names\n",
    "    \"\"\"\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    # low-level access to hdfs driver\n",
    "    hadoop = sc._gateway.jvm.org.apache.hadoop\n",
    "    path = hadoop.fs.Path(path)\n",
    "    config = hadoop.conf.Configuration()\n",
    "\n",
    "    status = hadoop.fs.FileSystem.get(config).listStatus(path)\n",
    "    return (path_status.getPath().getName() for path_status in status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute_hdfs_files(hdfs_path):\n",
    "    from pyspark.context import SparkContext\n",
    "    \"\"\"Distributes recursively a given directory in HDFS to Spark\n",
    "\n",
    "    Args:\n",
    "        hdfs_path (str): path to directory\n",
    "    \"\"\"\n",
    "    sc = SparkContext.getOrCreate()\n",
    "\n",
    "    for path_name in list_path_names(hdfs_path):\n",
    "        path = os.path.join(hdfs_path, path_name)\n",
    "        print(\"Distributing {}...\".format(path))\n",
    "        sc.addFile(path, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    spark_home = \"/usr/hdp/current/spark2-client\"\n",
    "    sys.path.insert(0, spark_home + \"/python\")\n",
    "    sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.4-src.zip'))\n",
    "    filename = os.path.join(spark_home, 'python/pyspark/shell.py')  \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def app(spark):\n",
    "    from pyspark.context import SparkContext\n",
    "    from pyspark.sql.functions import udf\n",
    "    import pyspark.sql.types as T\n",
    "    from pyspark.sql import Row\n",
    "    from pyspark.sql.types import FloatType\n",
    "    \n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    lines =  sc.textFile(\"hdfs:///tmp/testFilePySpark.txt\")\n",
    "    parts = lines.map(lambda l: l.split(\" \"))\n",
    "    p = parts.map(lambda p: Row(name=p[0],value=float(p[1])))\n",
    "    df = spark.createDataFrame(p)\n",
    "    udf_var = udf(lambda x: round(np.var(np.array(x)),10), returnType=FloatType()) \n",
    "    df = df.withColumn('WF_Var',udf_var('value'))\n",
    "    print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    filename = init()\n",
    "    \n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.context import SparkContext\n",
    "    spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        config(\"spark.app.name\",\"testest\").\\\n",
    "        config(\"spark.master\",\"yarn\").\\\n",
    "        config(\"spark.executor.memory\",\"2G\").\\\n",
    "        config(\"spark.executor.instances\",\"2\").\\\n",
    "        config(\"spark.driver.memory\",\"2G\").\\\n",
    "        config(\"spark.executor.cores\", \"2\"). \\\n",
    "        config(\"spark.driver.maxResultSize\", \"1G\").\\\n",
    "        config(\"spark.executor.extraJavaOptions\",\"-XX:+UseG1GC\").\\\n",
    "        getOrCreate()\n",
    "    exec(compile(open(filename, \"rb\").read(), filename, 'exec'))\n",
    "     \n",
    "    env_dir = 'hdfs://' + ENV_DIR\n",
    "    # make sure we have the latest version available on HDFS\n",
    "    distribute_hdfs_files(env_dir)\n",
    "    \n",
    "    \n",
    "    app(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.2.0.2.6.4.0-91\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.13 (default, Dec 20 2016 23:09:15)\n",
      "SparkSession available as 'spark'.\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/dateutil...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/numpy...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/numpy-1.12.1-cp27-cp27mu-manylinux1_x86_64.whl...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/numpy-1.12.1.data...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/numpy-1.12.1.dist-info...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/pandas...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/pandas-0.20.3-cp27-cp27mu-linux_x86_64.whl...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/pandas-0.20.3.dist-info...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/python_dateutil-2.6.0-py2.py3-none-any.whl...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/python_dateutil-2.6.0.dist-info...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/pytz...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/pytz-2017.2-py2.py3-none-any.whl...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/pytz-2017.2.dist-info...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/six-1.10.0-py2.py3-none-any.whl...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/six-1.10.0.dist-info...\n",
      "Distributing hdfs:///tmp/envs/py27PySpark/venv/six.py...\n",
      "+----+---------------+------+\n",
      "|name|          value|WF_Var|\n",
      "+----+---------------+------+\n",
      "| you|0.0432052044116|   0.0|\n",
      "|   i|0.0391075831328|   0.0|\n",
      "| the|0.0328010698268|   0.0|\n",
      "|  to|0.0237549924919|   0.0|\n",
      "|   a|0.0209682886489|   0.0|\n",
      "|  it|0.0198104294359|   0.0|\n",
      "+----+---------------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
